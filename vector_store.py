# -*- coding: utf-8 -*-
"""vector_store.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tv4WsVBoVe3MetDVvDHQxuOu7y9h8LnC
"""

import os,time

# !pip install langchain chromadb tiktoken pypdf  langchain-community langchain-huggingface

from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import PyPDFLoader
from langchain.schema import Document
from langchain.text_splitter import CharacterTextSplitter

# Inititalise the embedding model
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Point 1
doc1 = Document(
    page_content="LangChain is a framework for building apps using large language models (LLMs). It helps developers create powerful applications by connecting LLMs with external data, APIs, and tools. This makes it easier to build dynamic and interactive AI-powered systems.",
    metadata={
        "source": "langchain_introduction",
        "category": "overview",

    }
)

# Point 2
doc2 = Document(
    page_content="It provides modular components like LLMs, prompts, chains, agents, and memory. These building blocks can be combined in different ways to create reusable and flexible workflows. This modularity allows for rapid development and customization of AI applications.",
    metadata={
        "source": "langchain_components",
        "category": "architecture",

    }
)

# Point 3
doc3 = Document(
    page_content="LangChain supports Retrieval-Augmented Generation (RAG) for data-aware AI. You can feed your own documents into the system so the model can answer based on your data. This makes it possible to build question-answering systems that use up-to-date or private information.",
    metadata={
        "source": "langchain_rag",
        "category": "use_case",

    }
)

# Point 4
doc4 = Document(
    page_content="It works with vector databases to store and search embeddings efficiently. Using embeddings from models like HuggingFace, you can store document vectors in systems like FAISS or Pinecone. During inference, relevant documents are retrieved quickly to support accurate responses.",
    metadata={
        "source": "langchain_vectorstores",
        "category": "data_integration",

    }
)

# Point 5
doc5 = Document(
    page_content="LangChain supports multiple LLM providers such as OpenAI, Anthropic, Google, and Hugging Face. You can switch between models or use local ones depending on your needs. This flexibility gives control over cost, performance, and privacy.",
    metadata={
        "source": "langchain_llm_support",
        "category": "integration"

    }
)

# Point 6
doc6 = Document(
    page_content="Itâ€™s widely used for real-world AI applications like chatbots, code assistants, and automation tools. From customer support bots to internal knowledge assistants, LangChain powers many practical use cases. Its active community and ecosystem make it a go-to tool for developers working with LLMs.",
    metadata={
        "source": "langchain_use_cases",
        "category": "applications",

    }
)

# Optional: Put all documents in a list
documents = [doc1, doc2, doc3, doc4, doc5, doc6]

documents

vector_store=Chroma(
    collection_name="langchain_docs",
    embedding_function=embeddings,
    persist_directory="my_chroma_db"
)

vector_store.add_documents(documents)

vector_store.get(include=['embeddings','documents'])

vector_store.similarity_search('How does LangChain support building AI apps with private data and multiple LLMs?',k=2)

retriever = vector_store.as_retriever()

relevant_docs = retriever.invoke("How does LangChain support building AI apps with private data and multiple LLMs?")

print("Relevant Documents:")
for i, doc in enumerate(relevant_docs):
    print(f"\nDocument {i+1} (Metadata: {doc.metadata}):\n{doc.page_content}")

vector_store.similarity_search_with_score('How can you build application using langchain',k=2)

vector_store.similarity_search_with_score(query="",filter={"category":"overview"})

updated_doc1=Document(
    page_content="LangChain allows the integration of external tools like databases, APIs, and Python functions to enhance LLM capabilities.",
    metadata={'source': 'langchain_introduction', 'category': 'overview'})

vector_store.update_document(document=updated_doc1,document_id='7cfd0e24-00e6-43fd-afd1-83b3f377c823')

vector_store.get()

"""## using Faiss Vector Store"""

from langchain.vectorstores import faiss

from langchain.embeddings import HuggingFaceEmbeddings

embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')

# Create LangChain documents for IPL players

doc1 = Document(
        page_content="Virat Kohli is one of the most successful and consistent batsmen in IPL history. Known for his aggressive batting style and fitness, he has led the Royal Challengers Bangalore in multiple seasons.",
        metadata={"team": "Royal Challengers Bangalore"}
    )
doc2 = Document(
        page_content="Rohit Sharma is the most successful captain in IPL history, leading Mumbai Indians to five titles. He's known for his calm demeanor and ability to play big innings under pressure.",
        metadata={"team": "Mumbai Indians"}
    )
doc3 = Document(
        page_content="MS Dhoni, famously known as Captain Cool, has led Chennai Super Kings to multiple IPL titles. His finishing skills, wicketkeeping, and leadership are legendary.",
        metadata={"team": "Chennai Super Kings"}
    )
doc4 = Document(
        page_content="Jasprit Bumrah is considered one of the best fast bowlers in T20 cricket. Playing for Mumbai Indians, he is known for his yorkers and death-over expertise.",
        metadata={"team": "Mumbai Indians"}
    )
doc5 = Document(
        page_content="Ravindra Jadeja is a dynamic all-rounder who contributes with both bat and ball. Representing Chennai Super Kings, his quick fielding and match-winning performances make him a key player.",
        metadata={"team": "Chennai Super Kings"}
    )

docs_list=[doc1,doc2,doc3,doc4,doc5]

docs_list

from langchain.vectorstores import FAISS

pip install faiss-cpu

faiss_vector_store=FAISS.from_documents(docs_list,embeddings)

retriever_faiss=faiss_vector_store.as_retriever(search_kwargs={'k':2})

retriever_faiss.invoke("who has led to most IPL wins in history")



