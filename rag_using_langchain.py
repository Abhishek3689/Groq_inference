# -*- coding: utf-8 -*-
"""RAG_using_langchain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n_i2dlj6LvMJmbgSlgxBHgIZHv9w7ita
"""

import os,getpass

from google.colab import userdata

GROQ_API_KEY = userdata.get('GROQ_API_KEY')
os.environ["GROQ_API_KEY"] = GROQ_API_KEY
print("API Key loaded successfully")

pip install langchain-groq

!pip install -q youtube-transcript-api langchain-community \
               faiss-cpu tiktoken python-dotenv

from langchain_core.prompts import PromptTemplate
from langchain_groq import ChatGroq

from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain_core.documents import  Document
from langchain.retrievers import MultiQueryRetriever

## initialize the model
llm=ChatGroq(
    model="llama3-8b-8192",
    temperature=0.3,
    max_tokens=None,
    timeout=None,
    max_retries=2,
)

## Check You tube transcript
video_id="6SO-8FcSkz4&t"

try:
  # if you dont care about langugae it gets the best one
  transcript_list=YouTubeTranscriptApi.get_transcript(video_id,languages=['en'])

  ## Flatten to get plain text
  transcript=' '.join([chunk['text'] for chunk in transcript_list])
  # Wrap text to fit terminal/screen width (default 80 chars)
  import textwrap
  wrapped_text = textwrap.fill(transcript, width=140)

  print(wrapped_text)

except TranscriptsDisabled:
  print("No caption availabe for this video")

transcript_list

"""## Step 1 Indexing"""

splitter=RecursiveCharacterTextSplitter(
    chunk_size=1500,
    chunk_overlap=200,

)

chunks=splitter.split_documents([Document(page_content=transcript)])

len(chunks)

chunks[0].page_content

## Get embedding model
embedding=HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

vector_store=FAISS.from_documents(
    embedding=embedding,
    documents=chunks,
)

vector_store.index_to_docstore_id

vector_store.get_by_ids(['2445896a-d1d5-459e-84eb-ce8ae1145ada'])

"""## Step 2 Retrieval"""

retriever=vector_store.as_retriever(search_type='similarity',search_kwargs=dict(k=4))

query="What is difference between Rag ,prompt engineering and fine tuning"

result=retriever.get_relevant_documents(query)

for doc in result:
  print(doc.page_content)
  print('-'*100)

"""## Augmentation"""

prompt = PromptTemplate(
    template="""
      You are a helpful assistant.
      Answer ONLY from the provided transcript context.
      If the context is insufficient, respond Answer is not availabe for this question.

      {context}
      Question: {question}
    """,
    input_variables = ['context', 'question'])

question=query
retrieved_docs=retriever.invoke(question)

for doc in retrieved_docs:
  print(doc.page_content)
  print('-'*100)

context_text='\n\n'.join([chunk.page_content for chunk in retrieved_docs])

print(context_text)

final_prompt=prompt.invoke({'context':context_text,'question':question})

final_prompt

"""## Step 4 Generation"""

answer=llm.invoke(final_prompt)
print(answer.content)

"""## Using Chain to get response"""

from langchain_core.output_parsers import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough,RunnableParallel,RunnableLambda

retriever=vector_store.as_retriever(search_type='mmr',search_kwargs=dict(k=4))
#

result=retriever.invoke(query)

for doc in result:
  print(doc.page_content)
  print('-'*100)

parser=StrOutputParser()

def cleaned_text(text):
  cleaned_txt='\n\n'.join(chunk.page_content for chunk in text)
  return cleaned_txt

parallel_chain=RunnableParallel(
   {'context':retriever | RunnableLambda(cleaned_text),
    'question':RunnablePassthrough()}
)

chain = parallel_chain | prompt | llm | parser

result=chain.invoke(query)

print(result)

result=chain.invoke("Can you summarize the video")

print(result)

